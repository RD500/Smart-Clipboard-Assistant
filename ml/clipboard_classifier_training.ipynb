{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "import pandas as pd\n",
        "print(\"Please upload your csv file.\")\n",
        "uploaded = files.upload()\n",
        "file_name = next(iter(uploaded))\n",
        "df = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
        "print(f\"\\nSuccessfully loaded {file_name}!\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "cq2NUlLH3aH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow>=2.12.0 tflite-support>=0.4.3 transformers>=4.26.0\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "# --- FIX: Remove duplicate rows from the DataFrame ---\n",
        "print(f\"Original number of rows: {len(df)}\")\n",
        "df = df.drop_duplicates().reset_index(drop=True)\n",
        "print(f\"Number of rows after removing duplicates: {len(df)}\")\n",
        "\n",
        "# 1. Preprocess the Data\n",
        "label2id = {label: i for i, label in enumerate(df['label'].unique())}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "df['label_id'] = df['label'].map(label2id)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    df['text'], df['label_id'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 2. Set up the Tokenizer and Datasets\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=128)\n",
        "val_encodings = tokenizer(X_val.tolist(), truncation=True, padding=True, max_length=128)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train.tolist())).shuffle(1000).batch(16)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), y_val.tolist())).batch(16)\n",
        "\n",
        "# 3. Set up the DistilBERT Model\n",
        "num_labels = len(label2id)\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained(\n",
        "    'distilbert-base-uncased',\n",
        "    num_labels=num_labels,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    from_pt=True\n",
        ")\n",
        "\n",
        "# 4. Train the Model\n",
        "num_epochs = 5\n",
        "num_train_steps = len(train_dataset) * num_epochs\n",
        "\n",
        "loss = SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
        "model.optimizer.learning_rate.assign(1e-4)\n",
        "\n",
        "model.fit(train_dataset, validation_data=val_dataset, epochs=num_epochs)\n",
        "\n",
        "# === METADATA CREATION FOR ANDROID ===\n",
        "print(\"\\n--- Creating Android metadata files ---\")\n",
        "\n",
        "# Create labels.txt file\n",
        "labels_content = \"\\n\".join([id2label[i] for i in range(len(id2label))])\n",
        "with open('labels.txt', 'w') as f:\n",
        "    f.write(labels_content)\n",
        "print(\"Created labels.txt\")\n",
        "\n",
        "# Create vocab.txt from tokenizer\n",
        "vocab = tokenizer.get_vocab()\n",
        "sorted_vocab = sorted(vocab.items(), key=lambda x: x[1])\n",
        "vocab_content = \"\\n\".join([token for token, _ in sorted_vocab])\n",
        "with open('vocab.txt', 'w') as f:\n",
        "    f.write(vocab_content)\n",
        "print(\" Created vocab.txt\")\n",
        "\n",
        "class DistilBertWrapper(tf.keras.Model):\n",
        "    def __init__(self, distilbert_model):\n",
        "        super().__init__()\n",
        "        self.distilbert = distilbert_model\n",
        "\n",
        "    def call(self, ids, mask, segment_ids):\n",
        "        # DistilBERT doesn't use segment_ids, so we ignore it\n",
        "        outputs = self.distilbert({\"input_ids\": ids, \"attention_mask\": mask})\n",
        "        logits = outputs.logits\n",
        "        probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "        return probabilities\n",
        "\n",
        "wrapped_model = DistilBertWrapper(model)\n",
        "\n",
        "# CRITICAL: Define serving signature with CORRECT tensor names\n",
        "@tf.function(input_signature=[\n",
        "    tf.TensorSpec(shape=[1, 128], dtype=tf.int32, name=\"ids\"),  # Changed from input_ids\n",
        "    tf.TensorSpec(shape=[1, 128], dtype=tf.int32, name=\"mask\"),  # Changed from attention_mask\n",
        "    tf.TensorSpec(shape=[1, 128], dtype=tf.int32, name=\"segment_ids\")  # Changed from token_type_ids\n",
        "])\n",
        "def serving_function(ids, mask, segment_ids):\n",
        "    probabilities = wrapped_model(ids, mask, segment_ids)\n",
        "    return {'output': probabilities}\n",
        "\n",
        "print(\"Model wrapper created with tensor names: 'ids', 'mask', 'segment_ids'\")\n",
        "\n",
        "# === CONVERSION CODE ===\n",
        "# 5. Convert the Model to TFLite\n",
        "concrete_func = serving_function.get_concrete_function()\n",
        "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
        "\n",
        "# Convert and save without metadata first\n",
        "tflite_model = converter.convert()\n",
        "output_filename = 'distilbert_model_clean.tflite'\n",
        "\n",
        "with open(output_filename, 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(f\"Model converted successfully: {output_filename}\")\n",
        "print(f\" Tensor names are now BertNLClassifier-compatible!\")\n",
        "\n",
        "# Verify tensor names\n",
        "print(\"\\nVerifying tensor names...\")\n",
        "interpreter = tf.lite.Interpreter(model_path=output_filename)\n",
        "interpreter.allocate_tensors()\n",
        "input_details = interpreter.get_input_details()\n",
        "\n",
        "print(\"Input tensor names:\")\n",
        "for i, detail in enumerate(input_details):\n",
        "    print(f\"  [{i}] {detail['name']}\")\n",
        "\n",
        "expected_names = ['ids', 'mask', 'segment_ids']\n",
        "actual_names = [d['name'] for d in input_details]\n",
        "\n",
        "if all(name in actual_names for name in expected_names):\n",
        "    print(\"\\n SUCCESS! All tensor names are correct!\")\n",
        "else:\n",
        "    print(\"\\n WARNING: Tensor names may not be correct\")\n",
        "\n",
        "# Download files in Colab\n",
        "print(\"\\n Downloading files for Android deployment...\")\n",
        "from google.colab import files\n",
        "files.download(output_filename)\n",
        "files.download('labels.txt')\n",
        "files.download('vocab.txt')"
      ],
      "metadata": {
        "id": "p9Nwd04B9k8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick test function - add this to your notebook after training\n",
        "def test_model(text):\n",
        "    \"\"\"Quick test function for immediate testing\"\"\"\n",
        "    # Tokenize the input\n",
        "    encoded = tokenizer([text], truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = model(encoded)\n",
        "    logits = prediction.logits[0]\n",
        "\n",
        "    # Get predicted class and confidence\n",
        "    predicted_class_id = tf.argmax(logits).numpy()\n",
        "    confidence = tf.nn.softmax(logits).numpy()\n",
        "\n",
        "    predicted_label = id2label[predicted_class_id]\n",
        "    confidence_score = confidence[predicted_class_id]\n",
        "\n",
        "    print(f\"Text: '{text}'\")\n",
        "    print(f\"Predicted: {predicted_label}\")\n",
        "    print(f\"Confidence: {confidence_score:.4f}\")\n",
        "    print(\"All probabilities:\")\n",
        "    for i, prob in enumerate(confidence):\n",
        "        print(f\"  {id2label[i]}: {prob:.4f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# TEST YOUR MODEL NOW!\n",
        "print(\"=== TESTING YOUR MODEL ===\\n\")\n",
        "\n",
        "# Test with various inputs\n",
        "test_inputs = [\n",
        "    \"john.smith@gmail.com\",\n",
        "    \"555-123-4567\",\n",
        "    \"https://www.example.com\",\n",
        "    \"123 Oak Street, Boston MA 02101\",\n",
        "    \"+1-800-555-0199\",\n",
        "    \"https://github.com/user/repo\",\n",
        "    \"contact@company.org\",\n",
        "    \"PO Box 789, Seattle WA\"\n",
        "]\n",
        "\n",
        "for text in test_inputs:\n",
        "    test_model(text)\n",
        "\n",
        "# Interactive testing - you can modify these\n",
        "print(\"\\n=== TRY YOUR OWN INPUTS ===\")\n",
        "print(\"Change the text below and run to test:\")\n",
        "\n",
        "# CHANGE THESE TO TEST YOUR OWN INPUTS:\n",
        "my_test_1 = \"bhoomikasundar.cs23@rvce.edu.in\"\n",
        "my_test_2 = \"88891-60160\"\n",
        "my_test_3 = \"https://claude.ai/chat/deaa27fe-e7b5-4129-b015-4818da3826d5\"\n",
        "my_test_4 = \"221 B,Basavangudi,Bengaluru\"\n",
        "\n",
        "test_model(my_test_1)\n",
        "test_model(my_test_2)\n",
        "test_model(my_test_3)\n",
        "test_model(my_test_4)"
      ],
      "metadata": {
        "id": "Kq2t2lbVD8qn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}